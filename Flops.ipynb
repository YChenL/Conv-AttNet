{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d488cbd2-fdfd-4a56-aab4-a0bc78abe603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchstat import stat\n",
    "from models import attentions, ConvAttnNet\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "562e91e9-0a9a-4e85-8570-ac87e212c7b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "                 module name   input shape  output shape      params memory(MB)              MAdd             Flops  MemRead(B)  MemWrite(B) duration[%]    MemR+W(B)\n",
      "0                      conv1     3 224 224    64 112 112      9408.0       3.06     235,225,088.0     118,013,952.0    639744.0    3211264.0       0.76%    3851008.0\n",
      "1                        bn1    64 112 112    64 112 112       128.0       3.06       3,211,264.0       1,605,632.0   3211776.0    3211264.0       0.38%    6423040.0\n",
      "2                       relu    64 112 112    64 112 112         0.0       3.06         802,816.0         802,816.0   3211264.0    3211264.0       0.00%    6422528.0\n",
      "3                    maxpool    64 112 112    64  56  56         0.0       0.77       1,605,632.0         802,816.0   3211264.0     802816.0       0.38%    4014080.0\n",
      "4             layer1.0.conv1    64  56  56    64  56  56      4096.0       0.77      25,489,408.0      12,845,056.0    819200.0     802816.0       0.38%    1622016.0\n",
      "5               layer1.0.bn1    64  56  56    64  56  56       128.0       0.77         802,816.0         401,408.0    803328.0     802816.0       0.38%    1606144.0\n",
      "6             layer1.0.conv2    64  56  56    64  56  56     36864.0       0.77     231,010,304.0     115,605,504.0    950272.0     802816.0       0.38%    1753088.0\n",
      "7               layer1.0.bn2    64  56  56    64  56  56       128.0       0.77         802,816.0         401,408.0    803328.0     802816.0       0.00%    1606144.0\n",
      "8             layer1.0.conv3    64  56  56   256  56  56     16384.0       3.06     101,957,632.0      51,380,224.0    868352.0    3211264.0       0.76%    4079616.0\n",
      "9               layer1.0.bn3   256  56  56   256  56  56       512.0       3.06       3,211,264.0       1,605,632.0   3213312.0    3211264.0       0.00%    6424576.0\n",
      "10             layer1.0.relu   256  56  56   256  56  56         0.0       3.06         802,816.0         802,816.0   3211264.0    3211264.0       0.38%    6422528.0\n",
      "11     layer1.0.downsample.0    64  56  56   256  56  56     16384.0       3.06     101,957,632.0      51,380,224.0    868352.0    3211264.0       0.76%    4079616.0\n",
      "12     layer1.0.downsample.1   256  56  56   256  56  56       512.0       3.06       3,211,264.0       1,605,632.0   3213312.0    3211264.0       0.38%    6424576.0\n",
      "13            layer1.1.conv1   256  56  56    64  56  56     16384.0       0.77     102,559,744.0      51,380,224.0   3276800.0     802816.0       0.38%    4079616.0\n",
      "14              layer1.1.bn1    64  56  56    64  56  56       128.0       0.77         802,816.0         401,408.0    803328.0     802816.0       0.00%    1606144.0\n",
      "15            layer1.1.conv2    64  56  56    64  56  56     36864.0       0.77     231,010,304.0     115,605,504.0    950272.0     802816.0       0.38%    1753088.0\n",
      "16              layer1.1.bn2    64  56  56    64  56  56       128.0       0.77         802,816.0         401,408.0    803328.0     802816.0       0.00%    1606144.0\n",
      "17            layer1.1.conv3    64  56  56   256  56  56     16384.0       3.06     101,957,632.0      51,380,224.0    868352.0    3211264.0       0.76%    4079616.0\n",
      "18              layer1.1.bn3   256  56  56   256  56  56       512.0       3.06       3,211,264.0       1,605,632.0   3213312.0    3211264.0       0.38%    6424576.0\n",
      "19             layer1.1.relu   256  56  56   256  56  56         0.0       3.06         802,816.0         802,816.0   3211264.0    3211264.0       0.00%    6422528.0\n",
      "20            layer1.2.conv1   256  56  56    64  56  56     16384.0       0.77     102,559,744.0      51,380,224.0   3276800.0     802816.0       0.76%    4079616.0\n",
      "21              layer1.2.bn1    64  56  56    64  56  56       128.0       0.77         802,816.0         401,408.0    803328.0     802816.0       0.00%    1606144.0\n",
      "22            layer1.2.conv2    64  56  56    64  56  56     36864.0       0.77     231,010,304.0     115,605,504.0    950272.0     802816.0       0.76%    1753088.0\n",
      "23              layer1.2.bn2    64  56  56    64  56  56       128.0       0.77         802,816.0         401,408.0    803328.0     802816.0       0.00%    1606144.0\n",
      "24            layer1.2.conv3    64  56  56   256  56  56     16384.0       3.06     101,957,632.0      51,380,224.0    868352.0    3211264.0       0.76%    4079616.0\n",
      "25              layer1.2.bn3   256  56  56   256  56  56       512.0       3.06       3,211,264.0       1,605,632.0   3213312.0    3211264.0       0.38%    6424576.0\n",
      "26             layer1.2.relu   256  56  56   256  56  56         0.0       3.06         802,816.0         802,816.0   3211264.0    3211264.0       0.00%    6422528.0\n",
      "27            layer2.0.conv1   256  56  56   128  56  56     32768.0       1.53     205,119,488.0     102,760,448.0   3342336.0    1605632.0       0.76%    4947968.0\n",
      "28              layer2.0.bn1   128  56  56   128  56  56       256.0       1.53       1,605,632.0         802,816.0   1606656.0    1605632.0       0.38%    3212288.0\n",
      "29            layer2.0.conv2   128  56  56   128  28  28    147456.0       0.38     231,110,656.0     115,605,504.0   2195456.0     401408.0       0.38%    2596864.0\n",
      "30              layer2.0.bn2   128  28  28   128  28  28       256.0       0.38         401,408.0         200,704.0    402432.0     401408.0       0.38%     803840.0\n",
      "31            layer2.0.conv3   128  28  28   512  28  28     65536.0       1.53     102,359,040.0      51,380,224.0    663552.0    1605632.0       0.38%    2269184.0\n",
      "32              layer2.0.bn3   512  28  28   512  28  28      1024.0       1.53       1,605,632.0         802,816.0   1609728.0    1605632.0       0.00%    3215360.0\n",
      "33             layer2.0.relu   512  28  28   512  28  28         0.0       1.53         401,408.0         401,408.0   1605632.0    1605632.0       0.00%    3211264.0\n",
      "34     layer2.0.downsample.0   256  56  56   512  28  28    131072.0       1.53     205,119,488.0     102,760,448.0   3735552.0    1605632.0       1.15%    5341184.0\n",
      "35     layer2.0.downsample.1   512  28  28   512  28  28      1024.0       1.53       1,605,632.0         802,816.0   1609728.0    1605632.0       0.36%    3215360.0\n",
      "36            layer2.1.conv1   512  28  28   128  28  28     65536.0       0.38     102,660,096.0      51,380,224.0   1867776.0     401408.0       0.38%    2269184.0\n",
      "37              layer2.1.bn1   128  28  28   128  28  28       256.0       0.38         401,408.0         200,704.0    402432.0     401408.0       0.00%     803840.0\n",
      "38            layer2.1.conv2   128  28  28   128  28  28    147456.0       0.38     231,110,656.0     115,605,504.0    991232.0     401408.0       0.00%    1392640.0\n",
      "39              layer2.1.bn2   128  28  28   128  28  28       256.0       0.38         401,408.0         200,704.0    402432.0     401408.0       0.00%     803840.0\n",
      "40            layer2.1.conv3   128  28  28   512  28  28     65536.0       1.53     102,359,040.0      51,380,224.0    663552.0    1605632.0       0.76%    2269184.0\n",
      "41              layer2.1.bn3   512  28  28   512  28  28      1024.0       1.53       1,605,632.0         802,816.0   1609728.0    1605632.0       0.00%    3215360.0\n",
      "42             layer2.1.relu   512  28  28   512  28  28         0.0       1.53         401,408.0         401,408.0   1605632.0    1605632.0       0.00%    3211264.0\n",
      "43            layer2.2.conv1   512  28  28   128  28  28     65536.0       0.38     102,660,096.0      51,380,224.0   1867776.0     401408.0       0.38%    2269184.0\n",
      "44              layer2.2.bn1   128  28  28   128  28  28       256.0       0.38         401,408.0         200,704.0    402432.0     401408.0       0.00%     803840.0\n",
      "45            layer2.2.conv2   128  28  28   128  28  28    147456.0       0.38     231,110,656.0     115,605,504.0    991232.0     401408.0       0.38%    1392640.0\n",
      "46              layer2.2.bn2   128  28  28   128  28  28       256.0       0.38         401,408.0         200,704.0    402432.0     401408.0       0.39%     803840.0\n",
      "47            layer2.2.conv3   128  28  28   512  28  28     65536.0       1.53     102,359,040.0      51,380,224.0    663552.0    1605632.0       0.37%    2269184.0\n",
      "48              layer2.2.bn3   512  28  28   512  28  28      1024.0       1.53       1,605,632.0         802,816.0   1609728.0    1605632.0       0.38%    3215360.0\n",
      "49             layer2.2.relu   512  28  28   512  28  28         0.0       1.53         401,408.0         401,408.0   1605632.0    1605632.0       0.00%    3211264.0\n",
      "50            layer2.3.conv1   512  28  28   128  28  28     65536.0       0.38     102,660,096.0      51,380,224.0   1867776.0     401408.0       0.38%    2269184.0\n",
      "51              layer2.3.bn1   128  28  28   128  28  28       256.0       0.38         401,408.0         200,704.0    402432.0     401408.0       0.38%     803840.0\n",
      "52            layer2.3.conv2   128  28  28   128  28  28    147456.0       0.38     231,110,656.0     115,605,504.0    991232.0     401408.0       0.38%    1392640.0\n",
      "53              layer2.3.bn2   128  28  28   128  28  28       256.0       0.38         401,408.0         200,704.0    402432.0     401408.0       0.00%     803840.0\n",
      "54            layer2.3.conv3   128  28  28   512  28  28     65536.0       1.53     102,359,040.0      51,380,224.0    663552.0    1605632.0       0.76%    2269184.0\n",
      "55              layer2.3.bn3   512  28  28   512  28  28      1024.0       1.53       1,605,632.0         802,816.0   1609728.0    1605632.0       0.00%    3215360.0\n",
      "56             layer2.3.relu   512  28  28   512  28  28         0.0       1.53         401,408.0         401,408.0   1605632.0    1605632.0       0.00%    3211264.0\n",
      "57            layer2.4.conv1   512  28  28   128  28  28     65536.0       0.38     102,660,096.0      51,380,224.0   1867776.0     401408.0       0.38%    2269184.0\n",
      "58              layer2.4.bn1   128  28  28   128  28  28       256.0       0.38         401,408.0         200,704.0    402432.0     401408.0       0.00%     803840.0\n",
      "59            layer2.4.conv2   128  28  28   128  28  28    147456.0       0.38     231,110,656.0     115,605,504.0    991232.0     401408.0       0.38%    1392640.0\n",
      "60              layer2.4.bn2   128  28  28   128  28  28       256.0       0.38         401,408.0         200,704.0    402432.0     401408.0       0.38%     803840.0\n",
      "61            layer2.4.conv3   128  28  28   512  28  28     65536.0       1.53     102,359,040.0      51,380,224.0    663552.0    1605632.0       0.38%    2269184.0\n",
      "62              layer2.4.bn3   512  28  28   512  28  28      1024.0       1.53       1,605,632.0         802,816.0   1609728.0    1605632.0       0.38%    3215360.0\n",
      "63             layer2.4.relu   512  28  28   512  28  28         0.0       1.53         401,408.0         401,408.0   1605632.0    1605632.0       0.00%    3211264.0\n",
      "64            layer2.5.conv1   512  28  28   128  28  28     65536.0       0.38     102,660,096.0      51,380,224.0   1867776.0     401408.0       0.38%    2269184.0\n",
      "65              layer2.5.bn1   128  28  28   128  28  28       256.0       0.38         401,408.0         200,704.0    402432.0     401408.0       0.00%     803840.0\n",
      "66            layer2.5.conv2   128  28  28   128  28  28    147456.0       0.38     231,110,656.0     115,605,504.0    991232.0     401408.0       0.00%    1392640.0\n",
      "67              layer2.5.bn2   128  28  28   128  28  28       256.0       0.38         401,408.0         200,704.0    402432.0     401408.0       0.00%     803840.0\n",
      "68            layer2.5.conv3   128  28  28   512  28  28     65536.0       1.53     102,359,040.0      51,380,224.0    663552.0    1605632.0       0.38%    2269184.0\n",
      "69              layer2.5.bn3   512  28  28   512  28  28      1024.0       1.53       1,605,632.0         802,816.0   1609728.0    1605632.0       0.38%    3215360.0\n",
      "70             layer2.5.relu   512  28  28   512  28  28         0.0       1.53         401,408.0         401,408.0   1605632.0    1605632.0       0.00%    3211264.0\n",
      "71            layer2.6.conv1   512  28  28   128  28  28     65536.0       0.38     102,660,096.0      51,380,224.0   1867776.0     401408.0       0.38%    2269184.0\n",
      "72              layer2.6.bn1   128  28  28   128  28  28       256.0       0.38         401,408.0         200,704.0    402432.0     401408.0       0.00%     803840.0\n",
      "73            layer2.6.conv2   128  28  28   128  28  28    147456.0       0.38     231,110,656.0     115,605,504.0    991232.0     401408.0       0.38%    1392640.0\n",
      "74              layer2.6.bn2   128  28  28   128  28  28       256.0       0.38         401,408.0         200,704.0    402432.0     401408.0       0.00%     803840.0\n",
      "75            layer2.6.conv3   128  28  28   512  28  28     65536.0       1.53     102,359,040.0      51,380,224.0    663552.0    1605632.0       0.76%    2269184.0\n",
      "76              layer2.6.bn3   512  28  28   512  28  28      1024.0       1.53       1,605,632.0         802,816.0   1609728.0    1605632.0       0.00%    3215360.0\n",
      "77             layer2.6.relu   512  28  28   512  28  28         0.0       1.53         401,408.0         401,408.0   1605632.0    1605632.0       0.38%    3211264.0\n",
      "78            layer2.7.conv1   512  28  28   128  28  28     65536.0       0.38     102,660,096.0      51,380,224.0   1867776.0     401408.0       0.38%    2269184.0\n",
      "79              layer2.7.bn1   128  28  28   128  28  28       256.0       0.38         401,408.0         200,704.0    402432.0     401408.0       0.00%     803840.0\n",
      "80            layer2.7.conv2   128  28  28   128  28  28    147456.0       0.38     231,110,656.0     115,605,504.0    991232.0     401408.0       0.38%    1392640.0\n",
      "81              layer2.7.bn2   128  28  28   128  28  28       256.0       0.38         401,408.0         200,704.0    402432.0     401408.0       0.38%     803840.0\n",
      "82            layer2.7.conv3   128  28  28   512  28  28     65536.0       1.53     102,359,040.0      51,380,224.0    663552.0    1605632.0       0.38%    2269184.0\n",
      "83              layer2.7.bn3   512  28  28   512  28  28      1024.0       1.53       1,605,632.0         802,816.0   1609728.0    1605632.0       0.38%    3215360.0\n",
      "84             layer2.7.relu   512  28  28   512  28  28         0.0       1.53         401,408.0         401,408.0   1605632.0    1605632.0       0.00%    3211264.0\n",
      "85            layer3.0.conv1   512  28  28   256  28  28    131072.0       0.77     205,320,192.0     102,760,448.0   2129920.0     802816.0       0.76%    2932736.0\n",
      "86              layer3.0.bn1   256  28  28   256  28  28       512.0       0.77         802,816.0         401,408.0    804864.0     802816.0       0.00%    1607680.0\n",
      "87            layer3.0.conv2   256  28  28   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   3162112.0     200704.0       0.38%    3362816.0\n",
      "88              layer3.0.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "89            layer3.0.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.76%    2052096.0\n",
      "90              layer3.0.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.00%    1613824.0\n",
      "91             layer3.0.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.00%    1605632.0\n",
      "92     layer3.0.downsample.0   512  28  28  1024  14  14    524288.0       0.77     205,320,192.0     102,760,448.0   3702784.0     802816.0       0.76%    4505600.0\n",
      "93     layer3.0.downsample.1  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.38%    1613824.0\n",
      "94            layer3.1.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.38%    2052096.0\n",
      "95              layer3.1.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.38%     403456.0\n",
      "96            layer3.1.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.38%    2760704.0\n",
      "97              layer3.1.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.38%     403456.0\n",
      "98            layer3.1.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.38%    2052096.0\n",
      "99              layer3.1.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.38%    1613824.0\n",
      "100            layer3.1.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.00%    1605632.0\n",
      "101           layer3.2.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.38%    2052096.0\n",
      "102             layer3.2.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "103           layer3.2.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.38%    2760704.0\n",
      "104             layer3.2.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "105           layer3.2.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.38%    2052096.0\n",
      "106             layer3.2.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.00%    1613824.0\n",
      "107            layer3.2.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.00%    1605632.0\n",
      "108           layer3.3.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.38%    2052096.0\n",
      "109             layer3.3.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.38%     403456.0\n",
      "110           layer3.3.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.38%    2760704.0\n",
      "111             layer3.3.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.38%     403456.0\n",
      "112           layer3.3.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.38%    2052096.0\n",
      "113             layer3.3.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.00%    1613824.0\n",
      "114            layer3.3.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.38%    1605632.0\n",
      "115           layer3.4.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.38%    2052096.0\n",
      "116             layer3.4.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "117           layer3.4.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.76%    2760704.0\n",
      "118             layer3.4.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "119           layer3.4.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.38%    2052096.0\n",
      "120             layer3.4.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.38%    1613824.0\n",
      "121            layer3.4.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.00%    1605632.0\n",
      "122           layer3.5.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.39%    2052096.0\n",
      "123             layer3.5.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.37%     403456.0\n",
      "124           layer3.5.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.76%    2760704.0\n",
      "125             layer3.5.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "126           layer3.5.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.38%    2052096.0\n",
      "127             layer3.5.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.00%    1613824.0\n",
      "128            layer3.5.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.00%    1605632.0\n",
      "129           layer3.6.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.38%    2052096.0\n",
      "130             layer3.6.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.38%     403456.0\n",
      "131           layer3.6.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.38%    2760704.0\n",
      "132             layer3.6.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "133           layer3.6.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.38%    2052096.0\n",
      "134             layer3.6.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.38%    1613824.0\n",
      "135            layer3.6.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.00%    1605632.0\n",
      "136           layer3.7.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.76%    2052096.0\n",
      "137             layer3.7.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "138           layer3.7.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.76%    2760704.0\n",
      "139             layer3.7.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "140           layer3.7.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.38%    2052096.0\n",
      "141             layer3.7.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.38%    1613824.0\n",
      "142            layer3.7.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.00%    1605632.0\n",
      "143           layer3.8.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.38%    2052096.0\n",
      "144             layer3.8.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "145           layer3.8.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.38%    2760704.0\n",
      "146             layer3.8.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "147           layer3.8.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.38%    2052096.0\n",
      "148             layer3.8.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.00%    1613824.0\n",
      "149            layer3.8.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.00%    1605632.0\n",
      "150           layer3.9.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.38%    2052096.0\n",
      "151             layer3.9.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "152           layer3.9.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.76%    2760704.0\n",
      "153             layer3.9.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "154           layer3.9.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.76%    2052096.0\n",
      "155             layer3.9.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.00%    1613824.0\n",
      "156            layer3.9.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.00%    1605632.0\n",
      "157          layer3.10.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.76%    2052096.0\n",
      "158            layer3.10.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "159          layer3.10.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.76%    2760704.0\n",
      "160            layer3.10.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "161          layer3.10.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.76%    2052096.0\n",
      "162            layer3.10.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.00%    1613824.0\n",
      "163           layer3.10.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.00%    1605632.0\n",
      "164          layer3.11.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.38%    2052096.0\n",
      "165            layer3.11.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "166          layer3.11.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.38%    2760704.0\n",
      "167            layer3.11.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "168          layer3.11.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.38%    2052096.0\n",
      "169            layer3.11.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.00%    1613824.0\n",
      "170           layer3.11.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.38%    1605632.0\n",
      "171          layer3.12.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.38%    2052096.0\n",
      "172            layer3.12.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "173          layer3.12.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.38%    2760704.0\n",
      "174            layer3.12.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "175          layer3.12.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.76%    2052096.0\n",
      "176            layer3.12.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.39%    1613824.0\n",
      "177           layer3.12.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.00%    1605632.0\n",
      "178          layer3.13.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.37%    2052096.0\n",
      "179            layer3.13.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "180          layer3.13.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.38%    2760704.0\n",
      "181            layer3.13.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "182          layer3.13.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.38%    2052096.0\n",
      "183            layer3.13.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.38%    1613824.0\n",
      "184           layer3.13.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.00%    1605632.0\n",
      "185          layer3.14.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.38%    2052096.0\n",
      "186            layer3.14.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "187          layer3.14.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.36%    2760704.0\n",
      "188            layer3.14.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "189          layer3.14.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.76%    2052096.0\n",
      "190            layer3.14.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.00%    1613824.0\n",
      "191           layer3.14.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.00%    1605632.0\n",
      "192          layer3.15.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.38%    2052096.0\n",
      "193            layer3.15.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "194          layer3.15.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.38%    2760704.0\n",
      "195            layer3.15.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "196          layer3.15.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.38%    2052096.0\n",
      "197            layer3.15.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.38%    1613824.0\n",
      "198           layer3.15.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.00%    1605632.0\n",
      "199          layer3.16.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.38%    2052096.0\n",
      "200            layer3.16.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "201          layer3.16.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.38%    2760704.0\n",
      "202            layer3.16.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "203          layer3.16.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.38%    2052096.0\n",
      "204            layer3.16.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.00%    1613824.0\n",
      "205           layer3.16.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.38%    1605632.0\n",
      "206          layer3.17.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.38%    2052096.0\n",
      "207            layer3.17.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "208          layer3.17.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.38%    2760704.0\n",
      "209            layer3.17.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.38%     403456.0\n",
      "210          layer3.17.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.38%    2052096.0\n",
      "211            layer3.17.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.00%    1613824.0\n",
      "212           layer3.17.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.38%    1605632.0\n",
      "213          layer3.18.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.38%    2052096.0\n",
      "214            layer3.18.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "215          layer3.18.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.38%    2760704.0\n",
      "216            layer3.18.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.38%     403456.0\n",
      "217          layer3.18.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.38%    2052096.0\n",
      "218            layer3.18.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.38%    1613824.0\n",
      "219           layer3.18.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.00%    1605632.0\n",
      "220          layer3.19.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.38%    2052096.0\n",
      "221            layer3.19.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "222          layer3.19.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.38%    2760704.0\n",
      "223            layer3.19.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "224          layer3.19.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.38%    2052096.0\n",
      "225            layer3.19.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.38%    1613824.0\n",
      "226           layer3.19.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.00%    1605632.0\n",
      "227          layer3.20.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.38%    2052096.0\n",
      "228            layer3.20.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "229          layer3.20.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.38%    2760704.0\n",
      "230            layer3.20.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "231          layer3.20.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.38%    2052096.0\n",
      "232            layer3.20.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.00%    1613824.0\n",
      "233           layer3.20.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.38%    1605632.0\n",
      "234          layer3.21.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.38%    2052096.0\n",
      "235            layer3.21.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "236          layer3.21.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.38%    2760704.0\n",
      "237            layer3.21.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.38%     403456.0\n",
      "238          layer3.21.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.76%    2052096.0\n",
      "239            layer3.21.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.00%    1613824.0\n",
      "240           layer3.21.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.38%    1605632.0\n",
      "241          layer3.22.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.39%    2052096.0\n",
      "242            layer3.22.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "243          layer3.22.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.75%    2760704.0\n",
      "244            layer3.22.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "245          layer3.22.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.76%    2052096.0\n",
      "246            layer3.22.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.00%    1613824.0\n",
      "247           layer3.22.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.00%    1605632.0\n",
      "248          layer3.23.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.76%    2052096.0\n",
      "249            layer3.23.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "250          layer3.23.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.38%    2760704.0\n",
      "251            layer3.23.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.38%     403456.0\n",
      "252          layer3.23.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.38%    2052096.0\n",
      "253            layer3.23.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.38%    1613824.0\n",
      "254           layer3.23.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.00%    1605632.0\n",
      "255          layer3.24.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.19%    2052096.0\n",
      "256            layer3.24.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.38%     403456.0\n",
      "257          layer3.24.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.76%    2760704.0\n",
      "258            layer3.24.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "259          layer3.24.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.76%    2052096.0\n",
      "260            layer3.24.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.00%    1613824.0\n",
      "261           layer3.24.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.00%    1605632.0\n",
      "262          layer3.25.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.38%    2052096.0\n",
      "263            layer3.25.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "264          layer3.25.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.76%    2760704.0\n",
      "265            layer3.25.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "266          layer3.25.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.38%    2052096.0\n",
      "267            layer3.25.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.00%    1613824.0\n",
      "268           layer3.25.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.00%    1605632.0\n",
      "269          layer3.26.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.38%    2052096.0\n",
      "270            layer3.26.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.38%     403456.0\n",
      "271          layer3.26.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.38%    2760704.0\n",
      "272            layer3.26.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.38%     403456.0\n",
      "273          layer3.26.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.76%    2052096.0\n",
      "274            layer3.26.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.00%    1613824.0\n",
      "275           layer3.26.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.38%    1605632.0\n",
      "276          layer3.27.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.38%    2052096.0\n",
      "277            layer3.27.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "278          layer3.27.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.38%    2760704.0\n",
      "279            layer3.27.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.38%     403456.0\n",
      "280          layer3.27.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.76%    2052096.0\n",
      "281            layer3.27.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.00%    1613824.0\n",
      "282           layer3.27.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.00%    1605632.0\n",
      "283          layer3.28.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.38%    2052096.0\n",
      "284            layer3.28.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "285          layer3.28.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.76%    2760704.0\n",
      "286            layer3.28.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "287          layer3.28.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.38%    2052096.0\n",
      "288            layer3.28.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.00%    1613824.0\n",
      "289           layer3.28.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.00%    1605632.0\n",
      "290          layer3.29.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.39%    2052096.0\n",
      "291            layer3.29.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.36%     403456.0\n",
      "292          layer3.29.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.38%    2760704.0\n",
      "293            layer3.29.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "294          layer3.29.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.76%    2052096.0\n",
      "295            layer3.29.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.00%    1613824.0\n",
      "296           layer3.29.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.38%    1605632.0\n",
      "297          layer3.30.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.38%    2052096.0\n",
      "298            layer3.30.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "299          layer3.30.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.38%    2760704.0\n",
      "300            layer3.30.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "301          layer3.30.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.38%    2052096.0\n",
      "302            layer3.30.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.38%    1613824.0\n",
      "303           layer3.30.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.00%    1605632.0\n",
      "304          layer3.31.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.38%    2052096.0\n",
      "305            layer3.31.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.38%     403456.0\n",
      "306          layer3.31.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.76%    2760704.0\n",
      "307            layer3.31.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "308          layer3.31.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.38%    2052096.0\n",
      "309            layer3.31.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.00%    1613824.0\n",
      "310           layer3.31.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.00%    1605632.0\n",
      "311          layer3.32.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.38%    2052096.0\n",
      "312            layer3.32.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.38%     403456.0\n",
      "313          layer3.32.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.38%    2760704.0\n",
      "314            layer3.32.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.38%     403456.0\n",
      "315          layer3.32.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.40%    2052096.0\n",
      "316            layer3.32.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.35%    1613824.0\n",
      "317           layer3.32.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.00%    1605632.0\n",
      "318          layer3.33.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.38%    2052096.0\n",
      "319            layer3.33.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "320          layer3.33.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.38%    2760704.0\n",
      "321            layer3.33.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "322          layer3.33.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.38%    2052096.0\n",
      "323            layer3.33.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.00%    1613824.0\n",
      "324           layer3.33.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.00%    1605632.0\n",
      "325          layer3.34.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.00%    2052096.0\n",
      "326            layer3.34.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "327          layer3.34.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.38%    2760704.0\n",
      "328            layer3.34.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.38%     403456.0\n",
      "329          layer3.34.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.38%    2052096.0\n",
      "330            layer3.34.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.38%    1613824.0\n",
      "331           layer3.34.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.00%    1605632.0\n",
      "332          layer3.35.conv1  1024  14  14   256  14  14    262144.0       0.19     102,710,272.0      51,380,224.0   1851392.0     200704.0       0.38%    2052096.0\n",
      "333            layer3.35.bn1   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "334          layer3.35.conv2   256  14  14   256  14  14    589824.0       0.19     231,160,832.0     115,605,504.0   2560000.0     200704.0       0.76%    2760704.0\n",
      "335            layer3.35.bn2   256  14  14   256  14  14       512.0       0.19         200,704.0         100,352.0    202752.0     200704.0       0.00%     403456.0\n",
      "336          layer3.35.conv3   256  14  14  1024  14  14    262144.0       0.77     102,559,744.0      51,380,224.0   1249280.0     802816.0       0.76%    2052096.0\n",
      "337            layer3.35.bn3  1024  14  14  1024  14  14      2048.0       0.77         802,816.0         401,408.0    811008.0     802816.0       0.00%    1613824.0\n",
      "338           layer3.35.relu  1024  14  14  1024  14  14         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.00%    1605632.0\n",
      "339           layer4.0.conv1  1024  14  14   512  14  14    524288.0       0.38     205,420,544.0     102,760,448.0   2899968.0     401408.0       0.79%    3301376.0\n",
      "340             layer4.0.bn1   512  14  14   512  14  14      1024.0       0.38         401,408.0         200,704.0    405504.0     401408.0       0.00%     806912.0\n",
      "341           layer4.0.conv2   512  14  14   512   7   7   2359296.0       0.10     231,185,920.0     115,605,504.0   9838592.0     100352.0       0.76%    9938944.0\n",
      "342             layer4.0.bn2   512   7   7   512   7   7      1024.0       0.10         100,352.0          50,176.0    104448.0     100352.0       0.00%     204800.0\n",
      "343           layer4.0.conv3   512   7   7  2048   7   7   1048576.0       0.38     102,660,096.0      51,380,224.0   4294656.0     401408.0       0.76%    4696064.0\n",
      "344             layer4.0.bn3  2048   7   7  2048   7   7      4096.0       0.38         401,408.0         200,704.0    417792.0     401408.0       0.00%     819200.0\n",
      "345            layer4.0.relu  2048   7   7  2048   7   7         0.0       0.38         100,352.0         100,352.0    401408.0     401408.0       0.00%     802816.0\n",
      "346    layer4.0.downsample.0  1024  14  14  2048   7   7   2097152.0       0.38     205,420,544.0     102,760,448.0   9191424.0     401408.0       1.13%    9592832.0\n",
      "347    layer4.0.downsample.1  2048   7   7  2048   7   7      4096.0       0.38         401,408.0         200,704.0    417792.0     401408.0       0.00%     819200.0\n",
      "348           layer4.1.conv1  2048   7   7   512   7   7   1048576.0       0.10     102,735,360.0      51,380,224.0   4595712.0     100352.0       0.76%    4696064.0\n",
      "349             layer4.1.bn1   512   7   7   512   7   7      1024.0       0.10         100,352.0          50,176.0    104448.0     100352.0       0.00%     204800.0\n",
      "350           layer4.1.conv2   512   7   7   512   7   7   2359296.0       0.10     231,185,920.0     115,605,504.0   9537536.0     100352.0       0.76%    9637888.0\n",
      "351             layer4.1.bn2   512   7   7   512   7   7      1024.0       0.10         100,352.0          50,176.0    104448.0     100352.0       0.00%     204800.0\n",
      "352           layer4.1.conv3   512   7   7  2048   7   7   1048576.0       0.38     102,660,096.0      51,380,224.0   4294656.0     401408.0       0.38%    4696064.0\n",
      "353             layer4.1.bn3  2048   7   7  2048   7   7      4096.0       0.38         401,408.0         200,704.0    417792.0     401408.0       0.00%     819200.0\n",
      "354            layer4.1.relu  2048   7   7  2048   7   7         0.0       0.38         100,352.0         100,352.0    401408.0     401408.0       0.38%     802816.0\n",
      "355           layer4.2.conv1  2048   7   7   512   7   7   1048576.0       0.10     102,735,360.0      51,380,224.0   4595712.0     100352.0       0.38%    4696064.0\n",
      "356             layer4.2.bn1   512   7   7   512   7   7      1024.0       0.10         100,352.0          50,176.0    104448.0     100352.0       0.00%     204800.0\n",
      "357           layer4.2.conv2   512   7   7   512   7   7   2359296.0       0.10     231,185,920.0     115,605,504.0   9537536.0     100352.0       1.13%    9637888.0\n",
      "358             layer4.2.bn2   512   7   7   512   7   7      1024.0       0.10         100,352.0          50,176.0    104448.0     100352.0       0.00%     204800.0\n",
      "359           layer4.2.conv3   512   7   7  2048   7   7   1048576.0       0.38     102,660,096.0      51,380,224.0   4294656.0     401408.0       0.77%    4696064.0\n",
      "360             layer4.2.bn3  2048   7   7  2048   7   7      4096.0       0.38         401,408.0         200,704.0    417792.0     401408.0       0.00%     819200.0\n",
      "361            layer4.2.relu  2048   7   7  2048   7   7         0.0       0.38         100,352.0         100,352.0    401408.0     401408.0       0.36%     802816.0\n",
      "362                  avgpool  2048   7   7  2048   1   1         0.0       0.01               0.0               0.0         0.0          0.0       0.00%          0.0\n",
      "363                       fc          2048          1000   2049000.0       0.00       4,095,000.0       2,048,000.0   8204192.0       4000.0       0.00%    8208192.0\n",
      "total                                                     60192808.0     226.06  23,110,469,144.0  11,573,486,592.0   8204192.0       4000.0     100.00%  715448384.0\n",
      "=====================================================================================================================================================================\n",
      "Total params: 60,192,808\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total memory: 226.06MB\n",
      "Total MAdd: 23.11GMAdd\n",
      "Total Flops: 11.57GFlops\n",
      "Total MemR+W: 682.3MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet152()\n",
    "stat(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c198c371-b7ce-4790-b332-d8a76c026cb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dnn_18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54a8be36-d450-4ce3-930d-9041ce028055",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\pytorch19\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "model_dnn18 = AlterNet.dnn_18(num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d2faa5f-2933-4acb-9afb-3abef47d832e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlterNet(\n",
      "  (layer0): StemB(\n",
      "    (layer0): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (shortcut): Sequential()\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Sequential(\n",
      "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (sd): Identity()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (shortcut): Sequential()\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Sequential(\n",
      "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (sd): Identity()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      )\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (conv2): Sequential(\n",
      "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (sd): Identity()\n",
      "    )\n",
      "    (1): AttentionBasicBlockB(\n",
      "      (shortcut): Sequential()\n",
      "      (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): GELU()\n",
      "      (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (attn): LocalAttention(\n",
      "        (attn): WindowAttention(\n",
      "          dim=128, window_size=[7, 7], num_heads=4\n",
      "          (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (sd): Identity()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      )\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (conv2): Sequential(\n",
      "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (sd): Identity()\n",
      "    )\n",
      "    (1): AttentionBasicBlockB(\n",
      "      (shortcut): Sequential()\n",
      "      (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): GELU()\n",
      "      (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (attn): LocalAttention(\n",
      "        (attn): WindowAttention(\n",
      "          dim=256, window_size=[7, 7], num_heads=8\n",
      "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (sd): Identity()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      )\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (conv2): Sequential(\n",
      "        (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (sd): Identity()\n",
      "    )\n",
      "    (1): AttentionBasicBlockB(\n",
      "      (shortcut): Sequential()\n",
      "      (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): GELU()\n",
      "      (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (attn): LocalAttention(\n",
      "        (attn): WindowAttention(\n",
      "          dim=512, window_size=[7, 7], num_heads=16\n",
      "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (sd): Identity()\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): BNGAPBlock(\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (gap): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (dense): Linear(in_features=512, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_dnn18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ea884e5-9e1b-475f-b92d-f90174044582",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn18_attn_layer1 = attentions.WindowAttention(dim=128, window_size=[7, 7], num_heads=4)\n",
    "dnn18_attn_layer2 = attentions.WindowAttention(dim=256, window_size=[7, 7], num_heads=8)\n",
    "dnn18_attn_layer3 = attentions.WindowAttention(dim=512, window_size=[7, 7], num_heads=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8a82681-06c2-493b-aecc-6584524c5b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3825920"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn18_attn_layer1.flops(49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e77e36ca-cdc2-4ce1-8d1b-0dda2e6b2b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14074368"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn18_attn_layer2.flops(49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3793b61-d181-4055-a115-74d35093435a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53838848"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn18_attn_layer3.flops(49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcdd9aff-beba-42e1-8a2e-d9a853f6d49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71739021, 605, 389, 605, 389, 605, 504)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3825920 + 14074368 + 53838848 - 115,605,504 - 115,605,504 - 115,605,504"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3b5bce-6197-41f1-a98d-a9b1b116161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "-275,077,376"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f0603d8-df9f-4e29-827a-ebe89e0578e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.545"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.82-0.275"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab4d46a-d4c3-4650-991d-421190fb9619",
   "metadata": {},
   "source": [
    "### Dnn_34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ca1babd-4138-4b43-9d8d-41f6af6b0a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dnn34 = AlterNet.dnn_34(num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d88bf39c-a7cd-4e67-9662-69c127c2c12f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlterNet(\n",
      "  (layer0): StemB(\n",
      "    (layer0): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (shortcut): Sequential()\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Sequential(\n",
      "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (sd): Identity()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (shortcut): Sequential()\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Sequential(\n",
      "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (sd): Identity()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      )\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (conv2): Sequential(\n",
      "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (sd): Identity()\n",
      "    )\n",
      "    (1): AttentionBasicBlockB(\n",
      "      (shortcut): Sequential()\n",
      "      (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): GELU()\n",
      "      (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (attn): LocalAttention(\n",
      "        (attn): WindowAttention(\n",
      "          dim=128, window_size=[7, 7], num_heads=4\n",
      "          (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (sd): Identity()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      )\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (conv2): Sequential(\n",
      "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (sd): Identity()\n",
      "    )\n",
      "    (1): AttentionBasicBlockB(\n",
      "      (shortcut): Sequential()\n",
      "      (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): GELU()\n",
      "      (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (attn): LocalAttention(\n",
      "        (attn): WindowAttention(\n",
      "          dim=256, window_size=[7, 7], num_heads=8\n",
      "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (sd): Identity()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      )\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (conv2): Sequential(\n",
      "        (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (sd): Identity()\n",
      "    )\n",
      "    (1): AttentionBasicBlockB(\n",
      "      (shortcut): Sequential()\n",
      "      (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): GELU()\n",
      "      (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (attn): LocalAttention(\n",
      "        (attn): WindowAttention(\n",
      "          dim=512, window_size=[7, 7], num_heads=16\n",
      "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (sd): Identity()\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): BNGAPBlock(\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (gap): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (dense): Linear(in_features=512, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_dnn18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "709e9cc7-97c3-4799-ab88-6c230df236f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn34_attn_layer1 = attentions.WindowAttention(dim=128, window_size=[7, 7], num_heads=4)\n",
    "dnn34_attn_layer2 = attentions.WindowAttention(dim=256, window_size=[7, 7], num_heads=8)\n",
    "dnn34_attn_layer3 = attentions.WindowAttention(dim=512, window_size=[7, 7], num_heads=16)\n",
    "dnn34_attn_layer4 = attentions.WindowAttention(dim=512, window_size=[7, 7], num_heads=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bad0b75-0016-420f-81ab-5519a2f9ae43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3825920"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn34_attn_layer1.flops(49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "749cfa2f-e941-4102-a94f-40c45a2bfb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14074368"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn34_attn_layer2.flops(49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82b8d625-42b0-4734-85b8-b8dbf4a514cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53838848"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn34_attn_layer3.flops(49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43156117-31cb-42b4-8ae8-f774e906431c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53838848"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn34_attn_layer4.flops(49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c6f9840-00e5-40ff-ac41-b5b901bf1293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-336844032"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "14074368+3825920+53838848*2-115605504*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b85dbbf9-38b0-4669-9df5-02210a399740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3331999999999997"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3.67-0.3368"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ee789c-b116-4be7-a44f-18426d6e35d5",
   "metadata": {},
   "source": [
    "### Dnn_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5052e7a3-a1e3-4655-8630-1c1ea51b15ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dnn50 = AlterNet.dnn_50(num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd36c94a-47af-4264-8d34-ffd74ffb8e2d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlterNet(\n",
      "  (layer0): StemB(\n",
      "    (layer0): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (conv2): Sequential(\n",
      "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (conv3): Sequential(\n",
      "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (sd): Identity()\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (shortcut): Sequential()\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (conv2): Sequential(\n",
      "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (conv3): Sequential(\n",
      "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (sd): Identity()\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (shortcut): Sequential()\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (conv2): Sequential(\n",
      "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (conv3): Sequential(\n",
      "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (sd): Identity()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      )\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (conv2): Sequential(\n",
      "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (conv3): Sequential(\n",
      "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (sd): Identity()\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (shortcut): Sequential()\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (conv2): Sequential(\n",
      "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (conv3): Sequential(\n",
      "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (sd): Identity()\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (shortcut): Sequential()\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (conv2): Sequential(\n",
      "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (conv3): Sequential(\n",
      "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (sd): Identity()\n",
      "    )\n",
      "    (3): AttentionBlockB(\n",
      "      (shortcut): Sequential()\n",
      "      (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): GELU()\n",
      "      (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (attn): LocalAttention(\n",
      "        (attn): WindowAttention(\n",
      "          dim=128, window_size=[7, 7], num_heads=8\n",
      "          (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (sd): Identity()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      )\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (conv2): Sequential(\n",
      "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (conv3): Sequential(\n",
      "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (sd): Identity()\n",
      "    )\n",
      "    (1): AttentionBlockB(\n",
      "      (shortcut): Sequential()\n",
      "      (norm1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): GELU()\n",
      "      (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (attn): LocalAttention(\n",
      "        (attn): WindowAttention(\n",
      "          dim=256, window_size=[7, 7], num_heads=16\n",
      "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (sd): Identity()\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (shortcut): Sequential()\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (conv2): Sequential(\n",
      "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (conv3): Sequential(\n",
      "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (sd): Identity()\n",
      "    )\n",
      "    (3): AttentionBlockB(\n",
      "      (shortcut): Sequential()\n",
      "      (norm1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): GELU()\n",
      "      (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (attn): LocalAttention(\n",
      "        (attn): WindowAttention(\n",
      "          dim=256, window_size=[7, 7], num_heads=16\n",
      "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (sd): Identity()\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (shortcut): Sequential()\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (conv2): Sequential(\n",
      "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (conv3): Sequential(\n",
      "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (sd): Identity()\n",
      "    )\n",
      "    (5): AttentionBlockB(\n",
      "      (shortcut): Sequential()\n",
      "      (norm1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): GELU()\n",
      "      (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (attn): LocalAttention(\n",
      "        (attn): WindowAttention(\n",
      "          dim=256, window_size=[7, 7], num_heads=16\n",
      "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (sd): Identity()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      )\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (conv2): Sequential(\n",
      "        (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (conv3): Sequential(\n",
      "        (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (sd): Identity()\n",
      "    )\n",
      "    (1): AttentionBlockB(\n",
      "      (shortcut): Sequential()\n",
      "      (norm1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): GELU()\n",
      "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (attn): LocalAttention(\n",
      "        (attn): WindowAttention(\n",
      "          dim=512, window_size=[7, 7], num_heads=32\n",
      "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (sd): Identity()\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (shortcut): Sequential()\n",
      "      (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (conv2): Sequential(\n",
      "        (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (conv3): Sequential(\n",
      "        (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (sd): Identity()\n",
      "    )\n",
      "    (3): AttentionBlockB(\n",
      "      (shortcut): Sequential()\n",
      "      (norm1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): GELU()\n",
      "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (attn): LocalAttention(\n",
      "        (attn): WindowAttention(\n",
      "          dim=512, window_size=[7, 7], num_heads=32\n",
      "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (sd): Identity()\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): BNGAPBlock(\n",
      "      (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (gap): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (dense): Linear(in_features=2048, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_dnn50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ce8c92a-4135-4570-b8cb-aebec35f89dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7832"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4.12-0.3368"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b57de1b-bc56-46e2-9d7e-1a900fd5cfef",
   "metadata": {},
   "source": [
    "### Dnn_101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e296c9bb-0f4d-4b45-9364-b245e3fe8519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-398610688"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "14074368+3825920+53838848*3-115605504*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c69074fd-1094-4edf-81ab-235f8e773a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.4414"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7.84-0.3986"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4e99d04-6ba2-4051-8251-581f77569fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.1714"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11.57-0.3986"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d12c854d-7a1e-4bf6-83e7-67896c09a44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.layers as layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02330679-a307-4831-8341-0cdd2b452bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=nn.Sequential(nn.Linear(2048, 128), \n",
    "                    nn.Linear(128, 2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80961a1f-b7af-4c09-a0db-35a4eea189cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (262144x1 and 2048x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14472/3893118988.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2048\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch19\\lib\\site-packages\\torchstat\\statistics.py\u001b[0m in \u001b[0;36mstat\u001b[1;34m(model, input_size, query_granularity)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_granularity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[0mms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelStat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_granularity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0mms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch19\\lib\\site-packages\\torchstat\\statistics.py\u001b[0m in \u001b[0;36mshow_report\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mshow_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mcollected_nodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_analyze_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreport_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcollected_nodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch19\\lib\\site-packages\\torchstat\\statistics.py\u001b[0m in \u001b[0;36m_analyze_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_analyze_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mmodel_hook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelHook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[0mleaf_modules\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_hook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve_leaf_modules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mstat_tree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_leaf_modules_to_stat_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleaf_modules\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch19\\lib\\site-packages\\torchstat\\model_hook.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model, input_size)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_size\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# add module duration time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch19\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch19\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch19\\lib\\site-packages\\torchstat\\model_hook.py\u001b[0m in \u001b[0;36mwrap_call\u001b[1;34m(module, *input, **kwargs)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_origin_call\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             module.duration = torch.from_numpy(\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch19\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch19\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch19\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (262144x1 and 2048x128)"
     ]
    }
   ],
   "source": [
    "stat(model, (128,2048,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afa583bf-f5d5-4487-86b0-3017a754bc4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2048000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147b04b8-918d-4c31-a2a3-e1aee0619ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear(64,4)\n",
    "linear(4,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39cb9c7-10cd-4f03-bcd9-86d4224a8d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear(128,8)\n",
    "linear(8,128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
